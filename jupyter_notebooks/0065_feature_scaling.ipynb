{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Many machine learning algorithms work best when numerical data for each of the features (the characteristics such as petal length and sepal length in the iris data set) are on approximately the same scale. The conversion to a similar scale is called data normalisation or data scaling. We perform this as part of out data pre-processing before training an algorithm.\n",
    "\n",
    "Two common methods of scaling data are 1) to scale all data between 0 and 1 when 0 and 1 are the minimum and maximum values for each feature, and 2) scale the date so that the all features have a mean value of zero and a standard deviation of 1. We will perform this latter normalisation. Scikit learn will perform this for us, but the principle is simple: to normalise in this way, subtract the feature mean from all values, and then divide by the feature standard deviation.\n",
    "\n",
    "The normalisation parameters are established using the training data set. These parameters are then applied also to the test data set.\n",
    "\n",
    "Let's first import our packages including StandardScaler from sklearn.preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To imporve readability of the results, we'll set up a function to format NumPy output, controlling the width and decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_numpy_decimal_places(places, width=0):\n",
    "    set_np = '{0:' + str(width) + '.' + str(places) + 'f}'\n",
    "    np.set_printoptions(formatter={'float': lambda x: set_np.format(x)})\n",
    "    \n",
    "set_numpy_decimal_places(3,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up our iris data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "# Create training and test data sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "        X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now apply our scalar, training it on the training data set, and applying it to both the training and test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a new scaling object\n",
    "sc=StandardScaler() \n",
    "\n",
    "# Set up the scaler just on the training set\n",
    "sc.fit(X_train)\n",
    "\n",
    "# Apply the scaler to the training and test sets\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it has done to our data sets. Our training data set appears not to be perfectly normalised, but it is on exactly the same scale as the training data set, which is what our machine learning algorithms will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set data:\n",
      "Mean:  [ 5.893  3.045  3.829  1.227]\n",
      "StDev: [ 0.873  0.439  1.796  0.778]\n",
      "\n",
      "Scaled training set data:\n",
      "Mean:  [ 0.000 -0.000 -0.000 -0.000]\n",
      "StDev: [ 1.000  1.000  1.000  1.000]\n",
      "\n",
      "Original test set data:\n",
      "Mean:  [ 5.727  3.076  3.596  1.133]\n",
      "StDev: [ 0.688  0.414  1.656  0.715]\n",
      "\n",
      "Scaled test set data:\n",
      "Mean:  [-0.191  0.070 -0.130 -0.120]\n",
      "StDev: [ 0.789  0.943  0.922  0.919]\n"
     ]
    }
   ],
   "source": [
    "print ('Original training set data:')\n",
    "print ('Mean: ', X_train.mean(axis=0))\n",
    "print ('StDev:', X_train.std(axis=0))\n",
    "\n",
    "print ('\\nScaled training set data:')\n",
    "print ('Mean: ', X_train_std.mean(axis=0))\n",
    "print ('StDev:', X_train_std.std(axis=0))\n",
    "\n",
    "print ('\\nOriginal test set data:')\n",
    "print ('Mean: ', X_test.mean(axis=0))\n",
    "print ('StDev:', X_test.std(axis=0))\n",
    "\n",
    "print ('\\nScaled test set data:')\n",
    "print ('Mean: ', X_test_std.mean(axis=0))\n",
    "print ('StDev:', X_test_std.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
